{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#Change the location\n",
    "os.chdir(\"C:/Users/ahind/OneDrive/Desktop/Classes/Homework/Project\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\ahind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ahind\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "import re\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#import os\n",
    "#os.chdir(\"C:/Users/ahind/Downloads/\")\n",
    "from city_to_state import city_to_state_dict\n",
    "two_word_states=[\"New Hampshire\",\"New Jersey\",\"New Mexico\",\"New York\",\"North Carolina\",\"North Dakota\",\"Rhode Island\",\"South Carolina\",\"South Dakota\",\"West Virginia\"]\n",
    "import us\n",
    "nltk.download('stopwords')\n",
    "stopword = nltk.corpus.stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def cleanUpTweet(row):\n",
    "    import re\n",
    "    # Remove mentions\n",
    "    txt=row[\"text\"]\n",
    "    txt=txt.lower()\n",
    "    row[\"hashtags\"]=re.findall('(#[A-Za-z]+[A-Za-z0-9-_]+)', txt)\n",
    "    row[\"hashtag_length\"]=len(row[\"hashtags\"])\n",
    "    txt = re.sub(r'@[A-Za-z0-9_]+', '', txt)\n",
    "    # Remove hashtags\n",
    "    txt = re.sub(r'#[A-Z0-9]+', '', txt)\n",
    "    # Remove retweets:\n",
    "    txt = re.sub(r'RT : ', '', txt)\n",
    "    # Remove urls\n",
    "    txt = re.sub(r'https?:\\/\\/[A-Za-z0-9\\.\\/]+', '', txt)\n",
    "    #remove amp\n",
    "    txt = re.sub(r'&amp;', '', txt)\n",
    "    #rempve strange characters\n",
    "    txt = re.sub(r'ðŸ™', '', txt)\n",
    "    #remove new lines\n",
    "    txt = re.sub(r'\\n', ' ', txt)\n",
    "    #removing punctuations\n",
    "    txt = re.sub('[^\\w\\s]',' ',txt)\n",
    "    #remove digits\n",
    "    txt = re.sub('[d]+',' ',txt)\n",
    "    #keeping only alphabets\n",
    "    txt = re.sub(\"[^a-z]\",\" \", txt)\n",
    "    #tockenisation\n",
    "    text = re.split('\\W+', txt)\n",
    "   \n",
    "    text = [word for word in text if word not in stopword]\n",
    "    #stemming\n",
    "    ps = nltk.PorterStemmer()\n",
    "    row[\"tockens_without_stem\"]=text\n",
    "    row[\"tockens_without_stem\"]=[word for word in row[\"tockens_without_stem\"] if word not in \"\"]\n",
    "    \n",
    "    row[\"tockens\"] = [ps.stem(word) for word in text]\n",
    "    row[\"tockens\"]=[word for word in row[\"tockens\"] if word not in \"\"]\n",
    "    row[\"Cleaned_text\"]=\" \".join([word for word in row[\"tockens\"]])\n",
    "    return row\n",
    "\n",
    "def sentiment_ana(row):\n",
    "    #for index, row in tweets_df['Cleaned_Text'].iteritems():\n",
    "    text=row[\"Cleaned_text\"]\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(text)\n",
    "    if score['neg'] > score['pos']:\n",
    "        row[\"sentiment\"]= \"negative\"\n",
    "    elif score['pos'] > score['neg']:\n",
    "        row[\"sentiment\"]=\"positive\"\n",
    "    else:\n",
    "        row[\"sentiment\"]=\"neutral\"\n",
    "    \n",
    "\n",
    "    row[\"neg\"] = score['neg']\n",
    "    row[\"neu\"] = score['neu']\n",
    "    row[\"pos\"] = score['pos']\n",
    "    row[\"compound\"]= score['compound']\n",
    "    return row\n",
    "\n",
    "def loc_ana(loc):\n",
    "    x=loc\n",
    "    if re.match('({})'.format(\"|\".join(two_word_states)), x.lower()):\n",
    "        tokens = [re.match('({})'.format(\"|\".join(two_word_states)), x.lower()).group(0)]\n",
    "    elif re.match('({})'.format(\"|\".join(city_to_state_dict.keys()).lower()), x.lower()):\n",
    "        k = re.match('({})'.format(\"|\".join(city_to_state_dict.keys()).lower()), x.lower()).group(0)\n",
    "        tokens = [city_to_state_dict.get(k.title(), np.nan)]\n",
    "    else:\n",
    "        tokens = [j for j in re.split(\"\\s|,\", x) if j not in ['in', 'la', 'me', 'oh', 'or']]\n",
    "    for i in tokens:\n",
    "        if re.match('\\w+', str(i)):\n",
    "            if us.states.lookup(str(i)):\n",
    "                 return us.states.lookup(str(i)).abbr\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "chunksize = 1\n",
    "temp = 1\n",
    "cnt=1\n",
    "error_index=[]\n",
    "for chunk in pd.read_csv(\"data_2.csv\", chunksize=chunksize):\n",
    "    tweets_df=chunk\n",
    "    #Extracting only relevant columns\n",
    "    \n",
    "    tweets_df = tweets_df[[\"id\",\"created_at\",\"text\",\"user_location\"]]\n",
    "    tweets_df = tweets_df.rename(columns={\"created_at\": \"date\"})\n",
    "    tweets_df['date'] = pd.to_datetime(tweets_df['date']).dt.date\n",
    "    \n",
    "    \n",
    "        #Cleaning text\n",
    "    #print(cnt)\n",
    "    \n",
    "        #Applying sentiment\n",
    "    try:\n",
    "        tweets_df=tweets_df.apply(cleanUpTweet,axis=1)\n",
    "        \n",
    "        tweets_df=tweets_df.apply(sentiment_ana,axis=1)\n",
    "            #Extracting Location\n",
    "        tweets_df[\"user_location\"]=tweets_df[\"user_location\"].fillna(\" \")\n",
    "        tweets_df[\"Location_clean\"]=tweets_df[\"user_location\"].apply(loc_ana)\n",
    "    except:\n",
    "        error_index.append(cnt)\n",
    "        pass\n",
    "        \n",
    "    #chunk=chunk[[\"\"]]\n",
    "    if temp == 1:\n",
    "        temp = 2\n",
    "        tweets_df.to_csv('out_data_2_small.csv')\n",
    "    else:\n",
    "         tweets_df.to_csv('out_data_2_small.csv', mode='a', header=False)\n",
    "    cnt=cnt+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
